%% This is file `elsarticle-template-5-harv.tex',
%%
%% Copyright 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references
%%
%% $Id: elsarticle-template-5-harv.tex 159 2009-10-08 06:08:33Z rishi $
%% $URL: http://lenova.river-valley.com/svn/elsbst/trunk/elsarticle-template-5-harv.tex $
%%
\documentclass[preprint,authoryear,12pt]{elsarticle}
%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,authoryear,1p,times]{elsarticle}
%% \documentclass[final,authoryear,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,authoryear,3p,times]{elsarticle}
%% \documentclass[final,authoryear,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,authoryear,5p,times]{elsarticle}
%% \documentclass[final,authoryear,5p,times,twocolumn]{elsarticle}

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%% \usepackage{graphics}
%% or use the graphicx package for more complicated commands
%% \usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}
\usepackage{color}
\usepackage{siunitx}
\usepackage{array}

\newcommand{\rpm}{\raisebox{.2ex}{$\scriptstyle\pm$}}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon (default)
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   authoryear - selects author-year citations (default)
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%   longnamesfirst  -  makes first citation full author list
%%
%% \biboptions{longnamesfirst,comma}

% \biboptions{}

\journal{Biosystems Engineering}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

% \title{An Autonomous Platform for Use in Kiwifruit Orchards}
\title{A Platform for Autonomous Navigation of Kiwifruit Orchards}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

% \author{Mark H. Jones, Jamie Bell, Matthew Seabright, Joshua Barnett, Alistair Scarfe, Bruce MacDonald, Mike Duke}

%% Group authors per affiliation:
\author[UoW]{Mark H. Jones\corref{mjemail}}
\cortext[mjemail]{markj@waikato.ac.nz}

\author[UoA]{Jamie Bell\corref{jbemail}}
\cortext[jbemail]{jamie977@gmail.com}
\author[UoW]{Matthew Seabright}
\author[RPL]{Alistair Scarfe}
\author[UoW]{Mike Duke}
\author[UoA]{Bruce MacDonald}

\address[UoW]{School of Engineering, University of Waikato, Hamilton, New Zealand}
\address[UoA]{Faculty of Engineering, University of Auckland, Auckland, New Zealand}
\address[RPL]{Robotics Plus Ltd, Newnham Innovation Park, Tauranga, New Zealand}

\begin{abstract}
%% Text of abstract
    Systems for performing horticultural tasks usually require a means of locomotion through the environment.
    One approach is to directly integrate a drive system, thereby increasing the overall complexity and development risk of such a project.
    We present a generic platform designed to carry task specific robots through pergola style kiwifruit orchards.
    By separating the task of locomotion from other duties, a more modular and robust system overall is obtained.
    Sensors suited for autonomous navigation within this environment are selected and their suitability within a kiwifruit orchard is tested.
    Details of the platform's internal architecture is discussed.
    The presented platform has self-navigated through a test orchard unassisted and is capable of carrying a \SI{1000}{\kilo\gram} payload.
\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
    Agricultural automation \sep autonomous navigation \sep orchard robotics \sep sensor selection
\end{keyword}

\end{frontmatter}

% \linenumbers

%% main text
\section{Introduction}
\label{sect:intro}
    Short-term labor requirements within New Zealand's kiwifruit industry peak twice a year corresponding with the pollination and harvesting of kiwifruit.
    The majority of employment during these peaks is filled by seasonal or casual workers \citep{Timmins2009}.
    As kiwifruit is the country's largest horticultural export by value \citep{StatisticsNewZealand2015}, automation in this industry will promote economic growth.
    % The New Zealand government aims to double exports from its primary industries between 2012 and 2025 and is actively investing in programmes to achieve this \citep{MinistryPrimaryIndustries2015}.

    Previous work on automated kiwifruit harvesting has been demonstrated \citep{Scarfe2012}.
    That work presented a mobile platform with four integrated robot arms that were capable of harvesting kiwifruit from pergola style orchards.
    The platform presented here is a second generation of that unit.
    It increases modularity by separating the platform from the tasks it performs, namely harvesting and pollination.
    This work discusses only the base platform, where details of modules for pollination and harvesting are published separately \citep{williams2017,Seabright2017}.

    Automation in kiwifruit harvesting and pollinating demands computer control, state-of-the-art manipulators, and convolutional neural networks.
    These systems are bulky and have specific geometric requirements dictated by the environment and the tasks they perform.
    They share the requirements of transport to and from orchards, electrical power, and air pressure, but differ in they way they move when in the orchard.
    Pollinating modules, developed as part of a wider project, move at a well-known velocity with minimum changes in angle.
    This differs from the separately developed harvesting module that advances a set distance between stationary harvest cycles.
    % The duration of such harvesting cycles are determined by the number of fruit available during any particular cycle.
    As the harvester is designed to be autonomous, there must be communication between the platform and harvester to trigger forward movement between cycles.
    Therefore, to be truely autonomous, the harvesting module requires the platform it sits on to also be autonomous.

    It has been stated that ``since the robot development already includes a high complexity, the application itself should be of comparably low complexity'' \citep{Ruckelshausen2009}.
    By separating development of the base platform from the task-specific modules, risk of over-complexity is reduced by way of separation.
    The platform presented here simply needs to transport task specific modules autonomously through kiwifruit orchards.

    The development of autonomous vehicles in agriculture is not new, but much of the literature relates to manned vehicles converted to drive-by-wire.
    This work details the development of a purpose-built platform build for navigating kiwifruit orchards grown in a pregola style.

    \begin{figure}[htb]
        \centering
        \includegraphics[width=\linewidth]{imgs/photos/suzy_general.jpg}
        \caption{
            The robot platform driving through a pergola style kiwifruit orchard.
        }
        \label{fig:suzy}
    \end{figure}

\section{Related Work}
\label{sect:review}

    \subsection{Purpose-built Autonomous Vehicles in Agriculture}

        The introduction of computers and digital camera technology during the 1980s sparked research into autonomous vehicles for agricultural use \citep{Li2009}.
        When publishing details of an autonomous vehicle in 1999, Tillett et~al.\@ cites difficulties dealing with variability in lighting and the environment as the reason no commercial ready vehicles were available at the time.
        Their vehicle combined wheel encoders, a compass, and accelerometers for odometry information.
        It also featured a camera based row guidance system.
        It was capable of spraying individual plants whilst autonomously driving at a relatively slow pace of \SI{2.5}{\kilo\meter\per\hour}.

        Three years later, two autonomous robots designed for weed mapping and control were published \citep{Pedersen2002,Astrand2002}.
        These platforms had relatively simple chassis and drive systems as they were both at a prototype stage.
        They featured two wheel steering and were designed specifically for field crops.
        Both were battery powered, however the unit presented astrand could also be fitted with a combustion engine.
        The vehicle presented by \cite{Pedersen2002} was designed to follow pre-defined GPS based paths through row-crops, but the authors found that this was impractical without a dedicated row guidance sensor.
        They proposed a revised design that featured a row guidance sensor, a new drive system with four wheel steering, and a Controller Area Network (CAN) bus for low level communication.
        % This work demonstrates a need to combine data from multiple sensor types.
         % Controller Area Network (CAN) bus was used to communicate with drive and steering modules on the revised unit due to it being a dominating standard in agricultural vehicles.

        Two years later, the revised design proposed by \cite{Pedersen2002} is presented by \cite{Bak2004}.
        The authors noted that the control strategy for the four independently controlled wheels was non-trivial.
        The GPS receiver on this platform utilised Real Time Kinematic (RTK) corrections from a base station.
        RTK-GPS is capable of providing positioning with accuracies of \SI{2}{\centi\meter}.
        % Like the platform presented by Pederson et~al.\@, it combined a compass, gyroscope and GPS for odommetry.
        % However, it also featured the row detection sensor and a GPS receiver utilising Real Time Kinematic (RTK) corrections from a base station.
        
        % Their robot utilised a CAN bus for some aspects of system communication.

        % In 2008, Klose et~al.\@ publish details of `Weedy', a autonomous weed control robot for field use.
        % It used an overly simple four wheel steering geometry, likely to be a cost/complexity trade-off.
        % There are few details on the sensor selection apart from mention of the use of cameras and `acoustic distance sensors'.
        % Presumably the selection of drive geometry on this robot is a cost/complexity optimisation.
        % It too makes use of a CAN bus for communication between on-board modules.

        % The following year, many the same authors appearing on the `Weedy' paper published details an autonomous robotic platform with four wheel steering named BoniRob \citep{Ruckelshausen2009}.
        In 2009, details of BoniRob were published by \cite{Ruckelshausen2009}.
        Similar to the unit presented by \cite{Bak2004}, it features a gyroscope, RTK-GPS for localisation, a CAN bus for low level communication, and four wheel steering.
        What makes BoniRob particularly interesting is its ability to alter its track-width by actuating the arms to which the wheels were attached.
        It is battery powered, but can also be fitted with a \SI{2.8}{\kilo\watt} petrol generator.
        It is capable of carrying a \SI{150}{\kilo\gram} payload in its dedicated module space.
        Like the robots before it, BoniRob is designed for use on open field crops.
        It introduced the use of both single plane and multi-layer laser range scanning, known as lidar, for perception and row detection.
        % A CAN bus is used to control the low level systems (such as the drive control) and ethernet connections for higher level communication.
        During the previous year, some of these authors published details of a much simpler robot named `Weedy' \citep{Klose2008}, also an open field crop based sensing platform.

        Of particular relevance to this work is that of Scarfe et~al.\@ on an autonomous kiwifruit picking robot \citep{scarfe2009, Scarfe2012}.
        That work involved the creation of a hydraulically driven platform with two wheel steering to which four fruit harvesting arms were integrated.
        It used an internal combustion engine for on-board power generation and was designed specifically for use in kiwifruit orchards.
        While its ability to navigate a kiwifruit orchard was not tested, it formed the foundation platform we present here.

        Most recently, \cite{Bawden2017} present their field crop robot - Agbot~II.
        For traction it uses two driven wheels in a differential drive configuration, with two castor wheels for support.
        It is battery powered and designed to autonomously return to a shipping container sized shelter with in-built solar powered charging station.
        The overall design is based on two side modules that contain most of the drive system, and a wide centre-piece that bridges the side modules.
        This centre-piece can be changed so as to alter the total width of the vehicle, and is designed to be specific to the environment and application.
        The choice of drive system geometry has considerably reduced the robot's complexity and weight over the designs previously discussed.

        \cite{Blackmore2007} envisaged significant reductions in production costs for agricultural robotics by re-purposing parts already in use in the agricultural and automotive industry.
        While not a physical component, the CAN bus is one such technology borrowed from the automotive industry aiding development of low-level communications.
        Many of the platforms reviewed, especially the more recent ones, made use of this protocol for real-time communication.
        Platforms designed for open field crops appear to favor four-wheel steering over two wheel steering - with the exception of the Agbot II.
        The use of simulation tools allowed the creators of BoniRob to develop and test their mobility system without physical hardware.


    \subsection{Sensors for Row Based Navigation in Orchards}

        Sensor combinations for orchard based row detection fall into two three categories; camera based, lidar based, or a combination of the two.
        The following section summarises a review of row detection efforts in orchards using these techniques.
        % Lidar come in two flavours: single-plane, and multi-layer.

        \cite{Subramanian2006} tested both camera based guidance and lidar (Sick LMS-200) based guidance systems in a citrus fruit orchard.
        Sensors were trialled separately on a tractor retrofitted with a fly-by-wire system.
        Their vehicle was able to navigate a small and simplified path using both machine vision and lidar based approaches at speeds of up to \SI{4.4}{\meter\per\second}.
        Lidar proved more accurate until the point at which data transfer rate became a limiting factor.
        The image based was favorable after this point.
        They suggest that combining the two systems would give more robust guidance as well as providing the ability to detect obstacles.
        No mention of the ability for the image based approach to cope with varying lighting conditions is made.

        \cite{Barawid2007} demonstrate the use of data from a single-plane lidar (Sick LMS-219) being used to control a drive-by-wire tractor through an orchard.
        Their results show real-time processing of lidar data is sufficient to navigate an orchard at \SI{0.36}{\meter\per\second} (\SI{1.3}{\kilo\meter\per\hour}).

        \cite{Hansen2011} show the use of a single-plane lidar (Sick LMS-200) for vehicle localisation in an orchard.
        Also in 2011, two groups publish work on the generation of centre-lines from camera data of orchard rows.
        \cite{He2011} uses traditional machine vision, where \cite{Torres2011} makes use of neural network based image processing.
        Both methods generate valid paths, alghouth He et~al.\@ note that theirs may not be suitable when the envirnoment background becomes complex.

        The work of \cite{Scarfe2012}, combined traditional camera based image processing techniques with a single-plane lidar (Sick LMS-111).
        The image based approach failed to cope with variability in lighting conditions, however the lidar proved useful for detecting the trunks and posts of the row.

        \cite{Freitas2012} focus on the detection of people and bins in the rows of an apple orchard using lidar (Sick LMS-291), a low-cost inertial measurement unit, and wheel encoders.
        Their algorithm was capable of detecting each obstacle class off-line from data captured in an apple orchard.

        \cite{Zhang2014} use a lidar (Hokuyo UTM-30LX) to generate maps of an apple orchard with the aid of artificial landmarks.
        They use an actuated single-plane lidar to generate multi-plane data for use in row and landmark sensing.
        Placing artificial land-marks in orchards is designed to reduce the effort required to create orchard maps for guidance systems.

        The following year, many of the same authors write about their autonomous vehicle \citep{Bergerman2015}.
        It describes a electric utility vehicle converted to fly-by-wire with the addition wheel encoders for odometry and a single-plane lidar (Sick LMS-111).
        While not able to detect obstacles in real-time, their previous work processing off-line data \citep{Freitas2012} has potential to be integrated with the addition of extra computing capability.

        Most recently, \cite{Sharifi2015} write about a method to generate a centre-line from an image an of an orchard rows.
        Like the work of \cite{He2011}, the technique offers a way to generate paths from a single camera image without resorting to neural networks.
        However, their future work focuses on increasing robustness to variations in lighting conditions, which indicates issues in this area.
        They state their system has use in being complementary to lidar based navigation.

        These works show that traditional image based processing for navigation fails when the scene becomes complex or the lighting varies.
        Combining cameras with neural network based processing increases the robustness to environmental complexities, such as light or clutter, at the expense of increased computation.
        The experiences of \cite{Scarfe2012} and similar indicate that a lidar outputs data that requires less post-processing to be robust.
        The use of lidar has seen two of the reported vehicles navigate autonomously through orchard environments, which is promising.

        % Common among these vehicles is the use of sensor fusion, whereby data from multiple sensors is merged and filtered.
        % This provides a way to combine the advantages of multiple sensor types, and the benefit of redundancy  into a single computation space.
        With regards to the use of RTK-GPS in perception based guidance systems, Slaughter et~al.\@ points out the trade-off of requiring an ``unobstructed ``view'' of the sky from all parts of the field'' \citep{Slaughter2008}.
        Additionally, multi-path signal propagation caused by nearby foliage or the geometry of the land itself presents its own mode of failure \citep{Durrant-Whyte2005}.
        This requirement can not be satisfied under the canopy of a kiwifruit orchard which are usually surrounded by tall wind-breaking hedges.
        A feasibility analysis by \cite{Pedersen2006} highlighted the use of RTK-GPS systems as a significant cost in yearly subscriptions alone.
        \cite{Torii2000} suggests a combination of both RTK-GPS and machine vision systems to be the most promising system going forward based on reductions in costs and increases in performance of these systems.
        While \cite{Li2009} concludes that either GPS and machine vision, or GPS and lidar will be used together as a development trend.


\section{Platform Design}

    \subsection{Vehicle Configuration}
    \label{sect:mechanical}

        \begin{figure}[htb]
            \centering
            \includegraphics[width=\linewidth]{imgs/profile_views/AMMP-All-Labelled.pdf}
            \caption{Profile drawings of the robotic platform with kiwifruit bin.}
            \label{fig:AMMP}
        \end{figure}

        Modules designed to be carried by the platform require clearance from the canopy, in addition to to height they occupy themselves.
        A kiwifruit canopy ranges in height between \SI{1300}{\milli\meter} and \SI{1700}{\milli\meter}.
        To maximise space available to these modules the platform must be low-slung at the point of attachment.
        Figure \ref{fig:AMMP} illustrates the platform's design, with module area allocated between markers `G' and `H' in the side view (top left).
        The top of chassis in this region is \SI{360}{\milli\meter} above the ground.

        The platform's steering geometry is Ackermann based.
        Steering angles are controlled using the front two wheels which are actuated independently by brushless AC motors.
        The ability to actuate the angles individually simplifies the mechanical geometry needed to coordinate steering, particularly at extreme steering angles.
        Both steered wheels have the freedom to rotate \SI{340}{\degree}, limited by a mechanical stop.
        This range of steering angle allows the vehicle to place the centre of rotation between its rear wheels.
        The vehicle's turning circle is approximately twice its length.
        Implementing four wheel steering would allow the centre of rotation to move to the centre of the vehicle, decreasing the turning circle to the total length of the vehicle.
        However, headlands in kiwifruit orchards are sized for tractors to turn between rows, tractors which use two-wheel steering geometries.
        Implementing a two wheel steering system removes the need to develop ``non-trivial'' control strategies and increases the usable area.
        A differential drive, or skid steer, system was expected to cause ground damage to a level considered unacceptable to orchard owners.

        Bin lifting forks are fitted to the area between the rear wheels.
        This area is sized to accommodate a standard kiwifruit bin.
        The lifter is actuated by two vertically mounted pneumatic cylinders and is controlled by a standard pneumatic valve block.
        The platform will have the ability to pick and place bins while operating in the orchard.

        Other than its tires, the platform has no suspension.
        It features a front pivoting axle that ensures that a minimum of three wheels are always in contact with the ground.
        Each wheel is mounted directly to a 40:1 fixed ratio planetary gearbox connected to a permanent magnet brushless AC motor.
        This specific gearbox-motor combination limits the platform to a maximum speed of \SI{10}{\kilo\meter\per\hour}.
        The lack of suspension is not an issue when operating in orchards at this speed.
        In total, the drive system can continuously deliver \SI{25.6}{\kilo\watt} of power and \SI{3.3}{\kilo\newton\meter} of torque.
        Based on these specifications it is capable of accelerating from a stand-still to its maximum speed at an incline of \SI{20}{\degree} whilst carrying a \SI{600}{\kilo\gram} payload in \SI{2.0}{\second}.

        A power generation unit including a petrol engine, air compressor, and electrical alternator is fitted at the front of the vehicle.
        The drive shafts of each are connected by a heavy-duty timing belt.
        The compressor and alternator are activated electronically by an embedded controller module.
        Fuel and compressed air tanks sit over the right-hand rear wheel, these can be seen in figure \ref{fig:suzy}.
        Battery modules attached to the sides of the chassis each house fifteen lithium-iron-phosphate batteries (thirty batteries in total).

        Unloaded, the machine has an estimated mass of \SI{850}{\kilo\gram}, including the power generation unit.
        It is capable of carrying a \SI{1000}{\kilo\gram} payload.
        The mass of a standard bin of kiwifruit can be as much as \SI{400}{\kilo\gram}, leaving \SI{600}{\kilo\gram} for modules.
        % With that load, the platform's maneuverability and ability to brake or accelerate was not noticeably affected.


\section{Navigation Sensors}
\label{sect:sensors}
    The choice of sensors incorporated into a platform determines which approaches are available for navigation and object detection.
    This section details the sensor selection and navigation algorithms specific for use in kiwifruit orchards.
    We begin by selecting a number of sensors considered appropriate for navigation related trials in orchards.
    Based on these trials, sensors performing well are further tested with prototype navigation and object detection algorithms.
    Finally, an evaluation of each sensor/algorithm is discussed in the context of orchard based navigation.
%     Sensor selection was a critical early step in the development of the navigation system for the AMMP because it dictated the direction of subsequent research. Four approaches were used in the process of sensor selection:
%     \begin{enumerate}
%         \item Conducting a literature survey of sensors previously used in orchard navigation and similar applications.
%         \item Surveying the features, specifications and cost of sensors available to purchase in order to perform a cost-benefit analysis.
%         \item Purchasing some sensors, based on the cost-benefit analysis, collecting data from the sensors in kiwifruit orchards and analysing the data to ensure that the sensors perform according to some minimum criteria in the orchard environment.
%         \item Developing simple algorithms for the sensor data to demonstrate that the sensors can perform the functions that they have been purchased for.
%     \end{enumerate}

% \subsection{Sensor Literature Survey}
%     From the existing literature, it seems that the existing sensors that have been commonly used for orchard navigation systems are cameras, 2D lidar, GPS, inertial sensors and encoders for odometry. Some examples are summarised in Table \ref{table:1}.

%     \begin{table}[h!]
%     \caption{Examples of sensors used in previous orchard navigation research.}
%     \centering
%     \begin{tabular}{ | m{3.2cm} | m{3.2cm}| m{6.2cm} | }
%     \hline
%     \textbf{Reference} & \textbf{Application} & \textbf{Sensors} \\
%     \hline
%     Subramanian et al., 2006 & Citrus grove navigation & Angled down 2D lidar and camera (640x480) separately; steering encoder \\
%     \hline
%     Barawid et al., 2007 & Autonomous tractor & Horizontal 2D lidar \\
%     \hline
%     Hansen et al., 2011 & Autonomous orchard vehicle & Horizontal 2D lidar, fibre optic gyroscope, odometry (encoders) \\
%     \hline
%     He et al., 2011 & Orchard row detection & Camera (640x480) \\
%     \hline
%     Torres- Sospedra and Nebot, 2011 & Orchard row detection & Camera (640x480) \\
%     \hline
%     Scarfe, 2012 & Kiwifruit harvesting robot & Horizontal 2D lidar, fluxgate compass \\
%     \hline
%     Freitas et al., 2012  & Orchard robot obstacle detection & Angled down 2D lidar, wheel and steering encoders, low cost IMU \\
%     \hline
%     Zhang et al., 2013  & Orchard robot row following & Rotated 2D lidar; lidar rotation, wheel and steering encoders \\
%     \hline
%     Bergerman et al., 2015 & Autonomous orchard vehicle & Horizontal 2D lidar, wheel and steering encoders \\
%     \hline
%     Bargoti et al., 2015 & Apple orchard localisation & Vertical 2D lidar, Global Positioning Inertial Navigation System \\
%     \hline
%     Jagbrant et al., 2015 & Almond orchard localisation & Vertical 2D lidar, Global Positioning Inertial Navigation System \\
%     \hline
%     Sharifi and Chen, 2015 & Orchard row detection & Camera \\
%     \hline
%     \end{tabular}
%     \label{table:1}
%     \end{table}

\subsection{Sensor Selection}

    As the drive motors have built-in wheel encoders, basic odometry data is already available on the platform.
    Encoders on driven wheels can give false readings if wheel slip occurs so can not be used for odomentry alone.
    However, the data they provide can be used to assist with mapping, localisation, and provide velocity feedback.

    Other sensors considered for inclusion are outlined in table \ref{table:sensor_comparison} with their associated issues.
    Factors for selecting the trialled sensors were each sensor's specific strengths and weaknesses, reported usage in literature for similar environments, and availability at a commercially viable price.
    This investigation highlighted both lidar and 2D-cameras as offering the most functionality for navigation and object detection.
    Time-of-flight cameras were a compelling option based on the cost-benefit analysis; especially if the less expensive units work in sunlight.
    Because localisation is such a key functionality, the performance of GPS has also been evaluated in a kiwifruit orchard.
    The following sections detail our experiences while trialling these sensors.

    \begin{table}[htbp]
        \centering
        \footnotesize
        \begin{tabular}{ l l}
            \textbf{Sensor Type}      &\textbf{Possible Issues} \\ \hline
            GPS receiver              & Prone to signal loss from surrounding foiliage\\  \hline
            Inertial Measurement Unit & Error accumulation and thermal drift\\ \hline
            Digital Compass           & Can be affected by nearby metal structures\\ \hline
            Encoder                   & Error accumulation \\ \hline
            Lidar                     & Reduced visibility in fog and heavy rain \\ \hline
            Time of Flight Camera     & Reduced visibility in sunlight, fog and heavy rain \\ \hline
            Camera                    & Reduced visibility in fog or direct sunlight \\ \hline
            Thermal Camera            & Reduced visibility in conditions of low thermal contrast\\ \hline
        \end{tabular}
        \label{table:sensor_comparison}
        \caption{Sensor types considered for inclusion on the platform.}
    \end{table}

% \subsection{Data Collection and Inspection}
 %    The sensors that seemed important to test, based on both the literature survey and the cost-benefit analysis were lidar, cameras and GPS.
	% In addition, time of flight cameras were considered, because they seemed to be a compelling option based on the cost-benefit analysis; especially if some of the less expensive models were found to work well in sunlight.
	% It was also decided that encoders would be tested in favour of using an IMU because the encoders were built into the AMMP motors.
	% Some data was collected from each sensor in order to decide which sensors to prototype algorithms for.

    \subsubsection{In-orchard GPS Evaluation}
        Two GPS modules were evaluated: a Ublox Neo-M8N module and an OmniSTAR 5120VBS with AX0 series antenna.
    	Both were connected to a Beaglebone Black single board computer via serial connection for data acquisition.
        The Ublox module was selected for its -167 dBm sensitivity, internal Low Noise Amplifier (LNA), and active circuitry for the 25mm square ceramic patch antenna.
        The OmniSTAR receiver was chosen for its high gain antenna (34 dB) which claims high multi-path rejection.

        The testing procedure first involved planning a path through a single row of a kiwifruit orchard and plotting it on a satellite map.
        Waypoints were placed along the row at the location of posts used to hold the canopy's structure.
        Relative distances between these waypoints were measured with a tape measure and recorded.
        The receivers were then tested separately over the course of approximately two hours \color{red} Jamie, does this sound about right? I'm assuming the time here \color{black}.
        Before testing, each unit was powered up and given \SI{30}{\minute} to initialise in an open area near the kiwifruit orchard.
        During testing, each unit was walked slowly along the pre-determined path with stops at each waypoint to provide time for a fix.
        The path was approximately \SI{500}{\meter} in length and took approximately \SI{15}{\minute} to complete, including stops at each waypoint.
        Waypoints were spaced approximately \SI{5.5}{\meter} apart along the selected orchard row.

        \begin{figure}[htb]
            \centering
            \includegraphics{imgs/gps_path/gps_path.pdf}
            \caption{
                Aerial view of a path through a kiwifruit orchard and the associated GPS data.
            }
            \label{fig:gpsResults}
        \end{figure}

        The path followed, and the corresponding GPS locations collected from the receivers, is presented in figure \ref{fig:gpsResults}.
        It should be noted that data has been recorded for the round-trip so represents two passes along the path.
        It was noticed during testing that the signal quality lights on both GPS receivers regularly indicated a loss of signal.


        The Omnistar receiver appears to track the approximate path well, but the data is sparse with regular loss of signal after entering the orchard.
    	The Ublox receiver collected more data than the Omnistar unit, but was much less accurate.
        It may be possible to use a unit such as the Omnistar, which provided more accurate but fewer readings, as a sanity check for an approximate location within an orchard.
        Overall the units could not be relied on for localisation under the kiwifruit canopy and therefore were not used.

    \subsubsection{In-orchard Lidar Evaluation}
        Three lidar were evaluated, two single-plane lidars and one multi-layer lidar.
        The two single-plane lidars were the Hokuyo UTM-30LX and a SICK LMS111.
        The multi-layer is a Velodyne VLP-16 which has 16 horizontal \SI{360}{\degree} planes spread over \SI{15}{\degree} vertically.
        Data was collected from each lidar by driving through orchard rows with the sensor placed midway between the ground and canopy (approximately \SI{0.8}{\meter} off the ground).

        \begin{figure}[htb]
            \centering
            \includegraphics[width=\linewidth]{imgs/canopy_data/canopy_data.pdf}
            \caption{
                Captured lidar data showing the points reflected by the canopy (indicated by red markers) and points from tree trunks and posts (green markers).
                The arrow indicates the position and heading of the platform.
            }
            \label{fig:canopyDataCloud}
        \end{figure}

        The intention was to use the lidar as a means of detecting structure defining features of the orchard, such as posts, trunks and hedges.
        Detecting these features should allow for row boundary detection, or general mapping and localisation.
        However, both single-plane lidar produced clouds of unstructured data amongst the structured features, as shown in Figure \ref{fig:canopyDataCloud}.
        This was caused by the lidar's scan plane intercepting with the canopy on convex slopes.
        Similarly this issue arose on concave slopes where the plane intercepted with the ground, as depicted in figure \ref{fig:concaveSlope}.

        \begin{figure}[htb]
            \centering
            \includegraphics[width=\linewidth]{imgs/concave_slope/concave_slope_v4.pdf}
            \caption{
                On concave slopes the lidar scan plane intercepts with the ground instead of trunks or posts.
                The dashed line shows a horizontal plane coming from the the lidar.
                Dotted lines represent the upper and lower layers taken from the multi-layer lidar.
            }
            \label{fig:concaveSlope}
        \end{figure}

        This issue was reduced by the use of a multi-layer lidar and post-processing.
        Because of the 16 layers, it is possible to select the scan layer that gives the most useful viewing range.
        Referring to figure \ref{fig:concaveSlope}, that would correspond to the dotted line above the horizontal which intercepts with a trunk.

        In this environment a single-plane lidar could still be used at relatively short range as an independent channel of processing for redundancy or obstacle detection.
        It was decided that a multi-layer lidar would be best suited for navigation due to its ability to to see more distant features on undulating ground in kiwifruit orchards.
        For the platform, a single-plane lidar has been fitted to the front of the vehicle an object detector for possible collisions, and a multi-layer lidar has been fitted for use in navigation and general object detection.


    \subsubsection{In-orchard Camera Evaluation}
        \label{sect:camera_evaluation}

        Three varieties of camera were tested: time-of-flight, 3D stereoscopic, and traditional 2D cameras.

        The time-of-flight based camera was the Basler TOF640-20GM-850NM.
        It provides range, intensity, and confidence data at a resolution of 640 by 480 pixels.
        This specific model of camera was chosen for testing as it had previously proved useful when collecting depth data of kiwifruit canopies.
        It had been operated under different lighting conditions and exhibited minimal occurrences of data washout.
        However, these tests revealed that in both direct sunlight and overcast conditions there was significant data loss.

        The 3D stereo camera tested was an Intel RealSense R200.
        It combines a stereo pair of infra-red cameras with a colour camera.
        Additionally, it features an infra-red projector as a means of adding texture to objects in its field of view to assist with stereo processing.
        The appealing characteristics of this sensor were its low cost and its claim of being long-range and able to work outdoors.
        However, in both overcast and sunny conditions it suffered from a complete loss of range data.

        Finally, 2D-cameras were trialled.
        These were the Basler Dart daA1600-60uc, Flir CM3-U3-13S2C-CS, and Logitech C920 cameras.
        Like the lidar tests, each camera was driven through the orchard at a height of \SI{0.8}{\meter} from the ground.
        The Logitech C920 suffered from significant motion blur.
        Being a consumer grade web-camera it does not provide a hardware trigger interface.
        A hardware trigger will become important if the camera is used in stereo vision applications.
        The Basler and Flir cameras both produced images of sufficient quality.
        The Basler offering was favored for its later model image sensor.

        Overall, traditional 2D camera images were the best suited for object detection and classification.
        This was verified by processing the data using readily accessible detection algorithms such as convolutional neural networks.

\subsection{Sensor Demonstration by Prototype Algorithm}

    Basic tests indicated that multi-layer lidar and 2D camera sensors were best suited as primary navigation sensors.
    Further testing by prototyping navigation algorithms were made to ensure the apparent sensor benefits translated into practical advantages.
    The goals for the navigation system are:

    \begin{itemize}
        \item object detection and classification,
        \item mapping and localisation, and
        \item row tracking.
    \end{itemize}
    To validate that the sensors could perform these functions, three prototype navigation algorithms have been created.

    \subsubsection{Object Detection and Classification}

        % Object detection and classification was firstly prototyped fully on the camera data.
        Based on existing success using Convolutional Neural Networks (CNN) for image processing tasks \citep{LeCun2015}, a camera based system was chosen for object detection and classification.
    	The network architecture chosen was FCN-8s \citep{long2015}.
        It is a neural network made of convolutional layers without fully connected layers.
        % convolutional layers at the input and fully connected layers at the output.
    	FCN-8s performs semantic segmentation, which is per pixel labelling of images.

        To train the FCN-8s network, the camera image dataset used to assess the camera performance in section \ref{sect:camera_evaluation} was hand labeled.
    	Labeling involved drawing object outlines in each image, filling those outlines with colours corresponding to the object type, and filling any non-labeled areas with black.
        Each image was then converted to a pallet indexed format.
    	An example of an original image and its corresponding label image is shown as figure \ref{fig:segImgLabelPair}.

        \begin{figure}[htb]
            \centering
            \includegraphics[width=\linewidth]{imgs/photos/segImgLabelPair_trimmed.png}
            % \includegraphics[width=\linewidth]{imgs/photos/segImg_sideBySide.png}
            \caption{
                An input image (top) and labelled output (bottom) for semantic segmentation.
            }
            \label{fig:segImgLabelPair}
        \end{figure}

        \begin{figure}[htb]
            \centering
            \includegraphics[width=\linewidth]{imgs/photos/semSegRowResults.png}
            \caption{
                An example result from the FCN-8 network, segmenting a kiwifruit orchard row.
            }
            \label{fig:semSegRowResults}
        \end{figure}

        Experimentation with labeling individual trees and posts in the kiwifruit orchard was performed.
    	However, labeling the entire tree-line as a single class gave more robust results.
    	Objects labeled for this algorithm are:
        \begin{enumerate}
        \item traversable space (labeled as red),
        \item treelines (labeled as green), and
        \item the end of the row (labeled as tan).
        \end{enumerate}
        Traversable space was defined as the ground area that the platform could drive directly to without collision.
        These labelled images were then used to train the FCN-8s network.
    	Sample output from the trained network is presented as figure \ref{fig:semSegRowResults}.
        \colorbox{yellow}{was this actually used to perform row following?}

    \subsubsection{Mapping and Localisation}
        An existing Simultaneous Localisation and Mapping (SLAM) package was used to test the multi-plane lidar.
        The package was Gmapping \citep{Grisetti2007}, implemented as a ROS package \citep{Gerkey2010}.
    	Required input for Gmapping is odometry data and a single plane of lidar data.
        The obometry data was provided by the platform's built-in wheel encoders and the Velodyne VLP-16 lidar was used as the lidar.
    	As this lidar has 16 scanning layers, a conversion was performed to produce the single plane of lidar data required by Gmapping.

        \begin{figure}[htb]
            \centering
            \includegraphics[width=\linewidth]{imgs/single_plane_extraction/single_plane_extraction.pdf}
            \caption{
                Processing of multi-layer lidar data into a single plane equivalent.
                Blue points are those selected by the algorithm for further processing, whereas red are rejected.
            }
            \label{fig:singlePlaneExtraction}
        \end{figure}

        The simplest conversion from multi-layer data to a single plane would be to select one of the available planes and discard the rest.
        However, using this approach the system would loose any benefit offered by the multiple scanning layers.
        Instead, filtering the multi-layer data into a single plane was done by examining data from the centre four layers at each azimuth.
        If the difference between all four points falls below a certain threshold, the closest point is returned.
        Alternatively, if the spread in points is above the threshold, no points are returned.
        This eliminates points from sloped or varying surfaces while still returning points from objects with vertical structure.
        The effect of this is that the orchard's structural elements are visible, but the ground and canopy are not.

        \begin{figure}[htb]
            \centering
            \includegraphics{imgs/gmapmap/gmapmap.pdf}
            \caption{
                A resulting SLAM based map of a kiwifruit orchard created using Gmapping.
                Odometry information has been taken from wheel encoders and a multi-layer lidar.
            }
            \label{fig:gmapmap}
        \end{figure}

        The resulting points are then fed into Gmapping as a single plane of data along with the platform's wheel encoder data.
        Figure~\ref{fig:singlePlaneExtraction} shows the ability of this method to filter structure data from canopy or ground reflections.
        Using this method, a SLAM based map of the orchard was created by driving the lidar through only four of the orchard's rows, presented as figure \ref{fig:gmapmap}.


    \subsubsection{Kiwifruit Orchard Row Tracking}
        \label{sect:row_tracking}

        Our final navigation test requires interpreting the orchard's structure for path generation.
        Using the single-plane lidar data extraction technique used previously for SLAM mapping, a row guidance system is tested.
        The algorithm also makes use of the on-board wheel encoders.
        Details of the algorithm used for this row navigation are published elsewhere \citep{Bell2016}.
        A key modification to that algorithm was the calculation of the angular offset of the robotâ€™s coordinate system from the row centre-line.
        This was necessary due to the lidar not being mounted on the centreline of the platform.

        A visualisation of data captured whilst navigating the kiwifruit orchard is presented as figure \ref{fig:lastLidarFrame}.
        This algorithm was used to autonomously navigate \SI{20}{\kilo\meter} of kiwifruit orchards the platform and two smaller test platforms.
        The method does not perform SLAM, so the visualised data represents only the current sensor input, i.e., previous sensor data is not considered.
        Figure \ref{fig:lastLidarFrame} shows that while the algorithm is not perfect, it does a reasonable job at identifying orchard structure.

        \begin{figure}[htb]
            \centering
            \includegraphics[width=\linewidth]{imgs/row_following/row_following_narrow.pdf}
            \caption{
                Row detection from multi-plane lidar data.
                Red points indicate non-structured data that have been ignored.
                Blue points indicate orchard structure data used for row navigation.
                Grey lines link orchard structure points by their nearest neighbours.
                The black arrow represents the centreline of the row current row.
            }
            \label{fig:lastLidarFrame}
        \end{figure}

\subsection{Sensor Selection Conclusions}
    The results from prototype algorithms indicate that the multi-layer lidar combined with encoder feedback are feasible selections for performing mapping and localisation.
    This sensor combination alone have proven successful autonomous driving in kiwifruit orchards.
    A combination of 2D cameras and neural networking was suitable for object detection and classification.
    The possibility of using this as a means of generating a path for row following is being tested and is likely to be published in future.

	% Object detection using a combination of colour cameras and neural networks in kiwifruit orchards .
	Based on our test results for the navigation system, it was decided to proceed with using cameras, 3D lidar and encoders as the primary sensors for the navigation system.
    The autonomous driving algorithm described here has been used to drive 20 km autonomously on three different robot platforms, including the AMMP, in two different real kiwifruit orchards. This driving was performed using just lidar and encoder data.



\section{Hardware and Software Architecture}
\label{sect:architecture}

    \begin{figure}[htb]
        \centering
        \includegraphics[width=\linewidth]{imgs/system_diagram/diagram_v3.pdf}
        \caption{Hardware level system diagram showing the types of interfaces and relative relations on the platform.}
        \label{fig:system_diagram}
    \end{figure}

    This section briefly explains the hardware and software architecture of the platform.
    Mostly the platform uses standardised protocols and software frameworks, but the general arrangement is explained.

    The platform is centrally controlled by an x86 based small form factor PC running the server version of Ubuntu 16.04.
    That computer is connected to both an on-board Ethernet network and a CAN bus.
    Low-level sub-systems on the platform, such as motor controllers and relays, are connected to the main PC via a CAN bus, as shown in figure \ref{fig:system_diagram}.
    
    A second computer dedicated to processing sensing data for autonomous navigation is connected to the platform's main PC via Ethernet.
    The open source Robotic Operating System (ROS) is used to facilitate communicate between both computers and between software nodes within each machine.
    Figure \ref{fig:system_diagram_software} shows a simplified passage of information entering through navigation sensors, being passed between various nodes, and finally being fed the the motor controllers.
    % For simplicity it omits interface nodes, those used solely to interface the device to the ROS network.

    Relay modules connected via CAN bus allow the main PC to toggle power to on-board power supplies, motor controllers, park brakes, and lights.
    The modules also monitor the timing of synchronisation messages transmitted by the main PC every \SI{20}{\milli\second} onto the bus.
    If the module does not receive a syncronisation message for \SI{100}{\milli\second} it will enter an error state.
    That state will result in the motor controllers and on-board power supplies being shut off and the park brakes engaged.

    \begin{figure}[htb]
        \centering
        \includegraphics[width=\linewidth]{imgs/system_diagram/software.pdf}
        \caption{High level system diagram showing software architecture in terms of the manual and autonomous drive system.}
        \label{fig:system_diagram_software}
    \end{figure}

    To maximise code reusablility, each device on the platform has its own ROS node dedicated to publishing device data or subscribing to generated device commands.
    Examples of such devices are CAN adapters, motor controllers, wireless controllers, lidar, and encoders.
    Nodes are also used to transform or perform calculations on available data as well as pass it between nodes written in either C++ or Python.

    In addition to the drive commands generated by the navigation system, a safety rated wireless controller lets the operator generate commands by joystick.
    The controller has a mode switch which allows the operator to select between joystick and autonomous operation.
    An emergency stop button on the controller allows the operator to stop the platform at any time.


\section{Autonomous Driving}
\label{sect:autonomous}
    Using the row following algorithm developed for sensor testing, map based autonomous navigation of a kiwifruit orchard was implemented.
    This implementation required two key additions in order for the platform to become fully autonomous.
    These were the detection of a row's end and a method for turning between rows.
    Using the developed navigation approaches, the platform presented here is able to navigate the test orchard block unassisted.
    The test block consists of approximately \SI{1}{\kilo\meter} of total traversable length spread over 10 rows.

    Detection of the end of a row was made by detecting a minimum volume of free space above the multi-layer lidar (Velodyne VLP-16).
    In simple terms, this equates to searching for a lack of canopy above the robot.
    This approach makes use of the multi-layer lidar, where layers above the horizontal are used as an `absence of canopy' detector.

    Turns between rows were performed by driving a sequence of set curvature movements while performing obstacle avoidance.
    The set movements were recorded in a map file which were carried out by the platform on a turn-by-turn basis.
    A turn sequence would have the platform drive at a given angle for a given distance or until a given turn angle had been reached.
    Each row turn could contain any number of turn sub-manovours.
    On completion of the turn, the row following mode was entered which then guides the platform through the row.
    Figure \ref{fig:suzy_turning} shows the platform performing a row-end turn under autonomous control.

    \begin{figure}[htb]
        \centering
        \includegraphics[width=\linewidth]{imgs/photos/suzy_turning.jpg}
        \caption{
            Photo of the platform autonomously performing a row-end turn.
        }
        \label{fig:suzy_turning}
    \end{figure}


\section{Conclusion}
    In this work, a platform designed specifically for driving through pergola style kiwifruit orchards has been presented.
    The platform has the capability to carry payloads of \SI{600}{\kilo\gram} through these orchards without performance degradation.
    Sensors suitable for autonomous navigation have been selected, trialled and demonstrated as being useful as a means of navigating in this environment.
    Convolutional neural networks applied to monocular image data proved to be a promising technique for row navigation and further work in this area is under-way.
    By processing multi-layer lidar data, the platform was able to successfully and reliably navigate an entire orchard block without intervention.


\section*{Acknowledgements}
This research was supported by the New Zealand Ministry for Business, Innovation and Employment (MBIE) on contract UOAX1414.
The authors acknowledge contributions from Phillip Ross, Gordon Neshausen, Josh Barnett, and Erin Simms who were involved with the design and fabrication of the platform.

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% References
%%
%% Following citation commands can be used in the body text:
%%
%%  \citet{key}  ==>>  Jones et al. (1990)
%%  \citep{key}  ==>>  (Jones et al., 1990)
%%
%% Multiple citations as normal:
%% \citep{key1,key2}         ==>> (Jones et al., 1990; Smith, 1989)
%%                            or  (Jones et al., 1990, 1991)
%%                            or  (Jones et al., 1990a,b)
%% \cite{key} is the equivalent of \citet{key} in author-year mode
%%
%% Full author lists may be forced with \citet* or \citep*, e.g.
%%   \citep*{key}            ==>> (Jones, Baker, and Williams, 1990)
%%
%% Optional notes as:
%%   \citep[chap. 2]{key}    ==>> (Jones et al., 1990, chap. 2)
%%   \citep[e.g.,][]{key}    ==>> (e.g., Jones et al., 1990)
%%   \citep[see][pg. 34]{key}==>> (see Jones et al., 1990, pg. 34)
%%  (Note: in standard LaTeX, only one note is allowed, after the ref.
%%   Here, one note is like the standard, two make pre- and post-notes.)
%%
%%   \citealt{key}          ==>> Jones et al. 1990
%%   \citealt*{key}         ==>> Jones, Baker, and Williams 1990
%%   \citealp{key}          ==>> Jones et al., 1990
%%   \citealp*{key}         ==>> Jones, Baker, and Williams, 1990
%%
%% Additional citation possibilities
%%   \citeauthor{key}       ==>> Jones et al.
%%   \citeauthor*{key}      ==>> Jones, Baker, and Williams
%%   \citeyear{key}         ==>> 1990
%%   \citeyearpar{key}      ==>> (1990)
%%   \citetext{priv. comm.} ==>> (priv. comm.)
%%   \citenum{key}          ==>> 11 [non-superscripted]
%% Note: full author lists depends on whether the bib style supports them;
%%       if not, the abbreviated list is printed even when full requested.
%%
%% For names like della Robbia at the start of a sentence, use
%%   \Citet{dRob98}         ==>> Della Robbia (1998)
%%   \Citep{dRob98}         ==>> (Della Robbia, 1998)
%%   \Citeauthor{dRob98}    ==>> Della Robbia


%% References with bibTeX database:

\bibliographystyle{model5-names}
\bibliography{bibliography_jamie,bibliography_mark}


%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model5-names.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have one of the following forms:
%%   \bibitem[Jones et al.(1990)]{key}...
%%   \bibitem[Jones et al.(1990)Jones, Baker, and Williams]{key}...
%%   \bibitem[Jones et al., 1990]{key}...
%%   \bibitem[\protect\citeauthoryear{Jones, Baker, and Williams}{Jones
%%       et al.}{1990}]{key}...
%%   \bibitem[\protect\citeauthoryear{Jones et al.}{1990}]{key}...
%%   \bibitem[\protect\astroncite{Jones et al.}{1990}]{key}...
%%   \bibitem[\protect\citename{Jones et al., }1990]{key}...
%%   \harvarditem[Jones et al.]{Jones, Baker, and Williams}{1990}{key}...
%%

% \bibitem[ ()]{}

% \end{thebibliography}

\end{document}

%%
%% End of file `elsarticle-template-5-harv.tex'.
