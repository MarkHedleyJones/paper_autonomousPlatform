@inproceedings{6697124,
abstract = {Rows of trees such as in orchards, planted in straight parallel lines can provide navigation cues for autonomous machines that operate in between them. When the tree canopies are well managed, tree rows appear similar to corridor walls and a simple 2D sensing scheme suffices. However, when the tree canopies are three dimensional, or ground vegetation occludes tree trunks, it is necessary to use a three dimensional sensing mode. An additional complication in prolific canopies is that GPS is not reliable and hence is not suitable to register data from sensors onboard a traversing vehicle. Here, we present a method to register 3D data from a lidar sensor onboard a vehicle that must accurately determine its pose relative to the rows. We first register point cloud into a common reference frame and then determine the position of tree rows and trunks in the vicinity to determine the vehicle pose. Our method is tested online and with data from commercial orchards. Experimental results show that the accuracy is sufficient to enable accurate traversal between tree rows even when tree canopies do not approximate planar walls.},
author = {Zhang, J and Chambers, A and Maeta, S and Bergerman, M and Singh, S},
booktitle = {2013 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2013.6697124},
issn = {2153-0858},
keywords = {computer graphics;mobile robots;optical informatio},
month = {nov},
pages = {5306--5313},
title = {{3D perception for accurate row following: Methodology and results}},
year = {2013}
}
@article{s110404086,
abstract = {One of the most important parts of an autonomous robot is to establish the path by which it should navigate in order to successfully achieve its goals. In the case of agricultural robotics, a procedure that determines this desired path can be useful. In this paper, a new virtual sensor is introduced in order to classify the elements of an orange grove. This proposed sensor will be based on a color CCD camera with auto iris lens which is in charge of doing the captures of the real environment and an ensemble of neural networks which processes the capture and differentiates each element of the image. Then, the Hough's transform and other operations will be applied in order to extract the desired path from the classification performed by the virtual sensory system. With this approach, the robotic system can correct its deviation with respect to the desired path. The results show that the sensory system properly classifies the elements of the grove and can set trajectory of the robot.},
author = {Torres-Sospedra, Joaquin and Nebot, Patricio},
doi = {10.3390/s110404086},
issn = {1424-8220},
journal = {Sensors},
number = {4},
pages = {4086--4103},
title = {{A New Approach to Visual-Based Sensory System for Navigation into Orange Groves}},
url = {http://www.mdpi.com/1424-8220/11/4/4086},
volume = {11},
year = {2011}
}
@article{Subramanian2006130,
abstract = {Current production navigation systems for agricultural vehicles rely on {\{}GPS{\}} as the primary sensor for steering control. In citrus groves, where the tree canopy frequently blocks the satellite signals to the {\{}GPS{\}} receiver, an alternative method is required. This paper discusses the development of an autonomous guidance system for use in a citrus grove. The vehicle used for this study was a common tractor. Machine vision and laser radar (ladar) were individually used for guidance and a rotary encoder was used to provide feedback on the steering angle. A {\{}PID{\}} controller was developed to minimize the path error. The vehicle's guidance accuracy was tested in flexible test paths constructed of common hay bales. Path tracking performance was observed. The guidance system guided the tractor automatically through straight and curved paths. An average error of 2.8 cm using machine vision guidance and an average error of 2.5 cm using ladar guidance was observed, when the vehicle was tested in a curved path at a speed of 3.1 m/s. The guidance system successfully guided the vehicle in a citrus grove alleyway. },
author = {Subramanian, Vijay and Burks, Thomas F and Arroyo, A A},
doi = {http://dx.doi.org/10.1016/j.compag.2006.06.001},
issn = {0168-1699},
journal = {Computers and Electronics in Agriculture},
keywords = {Automatic guidance,Control,Laser sensor,Machine vision},
number = {2},
pages = {130--143},
title = {{Development of machine vision and laser radar based autonomous vehicle guidance systems for citrus grove navigation}},
url = {http://www.sciencedirect.com/science/article/pii/S016816990600069X},
volume = {53},
year = {2006}
}
@inproceedings{7081155,
author = {Sharifi, M and Chen, XiaoQi},
booktitle = {2015 6th International Conference on Automation, Robotics and Applications (ICARA)},
doi = {10.1109/ICARA.2015.7081155},
keywords = {Hough transforms;agriculture;feature extraction;gr},
month = {feb},
pages = {251--255},
title = {{A novel vision based row guidance approach for navigation of agricultural mobile robots in orchards}},
year = {2015}
}
@book{Scarfe2012,
address = {Manawatu},
author = {Scarfe, Alistair J},
publisher = {Massey University},
title = {{Development of an autonomous kiwifruit harvester : a thesis presented in partial fulfilment of the requirements for the degree of Doctor of Philosophy in Industrial Automation at Massey University, Manawatu, New Zealand}},
year = {2012}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
issn = {0028-0836},
journal = {Nature},
month = {may},
number = {7553},
pages = {436--444},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
title = {{Deep learning}},
url = {http://dx.doi.org/10.1038/nature14539 http://10.0.4.14/nature14539},
volume = {521},
year = {2015}
}
@inbook{Jagbrant2015,
abstract = {In this paper we present an approach to tree recognition and localisation in orchard environments for tree-crop applications. The method builds on the natural structure of the orchard by first segmenting the data into individual trees using a Hidden Semi-Markov Model. Second, a descriptor for representing the characteristics of the trees is introduced, allowing a Hidden Markov Model based matching method to associate new observations with an existing map of the orchard. The localisation method is evaluated on a dataset collected in an almond orchard, showing good performance and robustness both to segmentation errors and measurement noise.},
address = {Cham},
author = {Jagbrant, Gustav and Underwood, James Patrick and Nieto, Juan and Sukkarieh, Salah},
booktitle = {Field and Service Robotics: Results of the 9th International Conference},
doi = {10.1007/978-3-319-07488-7_32},
editor = {Mejias, Luis and Corke, Peter and Roberts, Jonathan},
isbn = {978-3-319-07488-7},
pages = {469--483},
publisher = {Springer International Publishing},
title = {{LiDAR Based Tree and Platform Localisation in Almond Orchards}},
url = {https://doi.org/10.1007/978-3-319-07488-7{\_}32},
year = {2015}
}
@incollection{He2011,
address = {Berlin, Heidelberg},
author = {He, Bei and Liu, Gang and Ji, Ying and Si, Yongsheng and Gao, Rui},
booktitle = {Computer and Computing Technologies in Agriculture IV: 4th IFIP TC 12 Conference, CCTA 2010, Nanchang, China, October 22-25, 2010, Selected Papers, Part I},
doi = {10.1007/978-3-642-18333-1_19},
editor = {Li, Daoliang and Liu, Yande and Chen, Yingyi},
isbn = {978-3-642-18333-1},
pages = {138--148},
publisher = {Springer Berlin Heidelberg},
title = {{Auto Recognition of Navigation Path for Harvest Robot Based on Machine Vision}},
url = {http://dx.doi.org/10.1007/978-3-642-18333-1{\_}19},
year = {2011}
}
@inproceedings{5991403,
author = {Hansen, S and Bayramoglu, E and Andersen, J C and Ravn, O and Andersen, N and Poulsen, N K},
booktitle = {Proceedings of the 2011 American Control Conference},
doi = {10.1109/ACC.2011.5991403},
issn = {0743-1619},
keywords = {Kalman filters;least squares approximations;mobile},
month = {jun},
pages = {4679--4684},
title = {{Orchard navigation using derivative free Kalman filtering}},
year = {2011}
}
@article{4084563,
author = {Grisetti, G and Stachniss, C and Burgard, W},
doi = {10.1109/TRO.2006.889486},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
keywords = {SLAM (robots);mobile robots;particle filtering (nu},
month = {feb},
number = {1},
pages = {34--46},
title = {{Improved Techniques for Grid Mapping With Rao-Blackwellized Particle Filters}},
volume = {23},
year = {2007}
}
@inproceedings{6385638,
author = {Freitas, G and Hamner, B and Bergerman, M and Singh, S},
booktitle = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2012.6385638},
issn = {2153-0858},
keywords = {agriculture;collision avoidance;image classificati},
month = {oct},
pages = {3391--3398},
title = {{A practical obstacle detection system for autonomous orchard vehicles}},
year = {2012}
}
@article{article,
author = {{C. Barawid}, Oscar and Mizushima, Akira and Ishii, Kazunobu and Noguchi, Noboru},
journal = {Biosystems Engineering},
pages = {139--149},
title = {{Development of an Autonomous Navigation System using a Two-dimensional Laser Scanner in an Orchard Application}},
volume = {96},
year = {2007}
}
@article{7059344,
author = {Bergerman, M and Maeta, S M and Zhang, J and Freitas, G M and Hamner, B and Singh, S and Kantor, G},
doi = {10.1109/MRA.2014.2369292},
issn = {1070-9932},
journal = {IEEE Robotics Automation Magazine},
keywords = {agricultural machinery;agricultural products;farmi},
month = {mar},
number = {1},
pages = {54--63},
title = {{Robot Farmers: Autonomous Orchard Vehicles Help Tree Fruit Production}},
volume = {22},
year = {2015}
}
@inproceedings{7759120,
author = {Bell, J and MacDonald, B A and Ahn, H S},
booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2016.7759120},
keywords = {control engineering computing;feature extraction;g},
month = {oct},
pages = {640--645},
title = {{Row following in pergola structured orchards}},
year = {2016}
}
@article{ROB:ROB21583,
author = {Bargoti, Suchet and Underwood, James P and Nieto, Juan I and Sukkarieh, Salah},
doi = {10.1002/rob.21583},
issn = {1556-4967},
journal = {Journal of Field Robotics},
number = {8},
pages = {1075--1094},
title = {{A Pipeline for Trunk Detection in Trellis Structured Apple Orchards}},
url = {http://dx.doi.org/10.1002/rob.21583},
volume = {32},
year = {2015}
}
@misc{Gerkey,
author = {Gerkey, Brian},
title = {gmapping},
url = {http://wiki.ros.org/gmapping},
urldate = {July 30 2017}
}
